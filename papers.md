

* [Towards Unified INT8 Training for Convolutional Neural Network](http://arxiv.org/abs/1912.12607)

* ğŸ”‘ Key:   
  * Mainly Dealing with the Gradient Quantization
  * Empirical 4 Rules of Gradient
  * Theoretical Convergence Bound & 2 Principles
  * 2 Technique: Directional-Sensitive Gradient Clipping + Deviation Counteractive LR Scaling
* ğŸ“ Source:  
  * CVPR 2020 SenseTime + BUAA
* ğŸŒ± Motivation: 
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200417205828.png)
* ğŸ’Š Methodology:
  * Symmetric Uniform Quantization with Stochastic Rounding
  * Challenged for Quantizing Gradients
    * Small perturbation would affect **direction**
    * *Sharp and Wide Distributionï¼ˆUnlike Weight/Activationï¼‰*
    * Evolutionary: *As time goes on, even more sharp*
    * *Layer Depth: Closely related to network depth(shallower the layer is, distribution sharper)*
    * *Special Block: DW Layer, always sharp*
  * Theoretical Bound afftected by 3 Terms(mainly with Quantization Error & LR & L2-Norm)
    * Useful Tricks: 1. Min Q Error   2. Scale Down the LR
  * Directional Sensitive Gradient Clipping
    * Actually its just plain grad clipping
    * Find the Clipping Value: Cosine Distance instead of MSE(Avoid the magnitude of grad's effect)
  * Deviation Counteractive LR Scaling
    * balance the exponentially accumulated grad error(deviation) by **exponentially decreasing LR accordingly**
    * ```f(deviation) = max(e^(-\alpha*deviation), \beta)```
      * \beta controls the lower bound of lr
      * \alpha controls the decay degree
  * Stochastic Rounding
    * curandGenerator
    * Linear Congruential Generator, yield a sequence of pseudo randomized number
* ğŸ“ Exps:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200417212040.png)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200417212102.png)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200417212118.png)
* ğŸ’¡ Ideas: 
  * (Found with smaller LR, MobV2 training didn't crash,although perf. decay)
  * Deviation of grad *exponentially* accumulated since its propagated through layer




* [Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](http://arxiv.org/abs/1901.09504)
* ğŸ”‘ Key:   
  * Outlier Channel Splitting
* ğŸ“ Source:  
  * Zhiru
* ğŸŒ± Motivation: 
  * Post-training quantization follows bell-shaped distribution while hardware could better handle linear
    * so the outlier becomes a problem
* ğŸ’Š Methodology:
  * Duplicate Outliers channels, then halves its value \
  * Similar to ã€ŠNet2Netã€‹ Net2WiderNet
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200417213824.png)
  * Post-Quantization's mainstreamï¼ŒFirst Clippingï¼Œthen Sym-Linear-Quan
    * Activation Clipping - Use Subset of input sample
    * Earlier work: min L2 Norm of Quantization Error
    * ACIQ: fits a Gaussian and Laplacian,use the fitting curve analytical compute optimal threshold
    * SAWB: Linear extrapolate 6 dists
    * TensorRt: Profile the dist, min the KL Divergence between original and quantized dist



* [Training Quantized Network with Auxiliary Gradient Module]()
    * é¢å¤–çš„fullPrecisionæ¢¯åº¦æ¨¡å—(è§£å†³residueçš„skip connectionä¸å¥½å®šçš„é—®é¢˜ï¼Œç›®çš„å‰å‘å®Œå…¨fix point)ï¼Œæœ‰å‡ åˆ†ç”¨ä¸€ä¸ªFPå»æ æ†èµ·ä½æ¯”ç‰¹ç½‘ç»œçš„æ„å‘³
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210113140.png)
      * è¿™é‡Œçš„Adapteræ˜¯ä¸€ä¸ª1x1Conv
      * FHå…±äº«å·ç§¯å±‚å‚æ•°ï¼Œä½†æ˜¯æœ‰ç‹¬ç«‹çš„BNï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­Jointly Optimize
      * ç”¨Hè¿™ä¸ªç½‘ç»œè·³è¿‡Shortcutï¼Œè®©æ¢¯åº¦å›æµ
    * å¥½åƒæ˜¯ä¸€ä¸ªå¾ˆå†—é•¿çš„æ–¹æ³•...


* [Scalable Methods for 8-bit Training of Neural Networks]()
  * Intel (AIPG)
  * æ¶‰åŠåˆ°äº†å‡ ç¯‡å…¶ä»–çš„å·¥ä½œ
    * [Mixed Precision Training of Convnet](https://arxiv.org/pdf/1802.00930.pdf)
      * 16bitè®­ç»ƒï¼Œæ²¡æœ‰ç²¾åº¦æŸå¤±
      * ç”¨äº†DFP (Dynamic Fixed Point)
    * [L1-Norm Batch Normalization for Efficient Training of Deep Neural Networks Shuang](https://arxiv.org/pdf/1802.09769.pdf)
  * RangeBN
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210171314.png)
    * (~~æ‰€ä»¥æˆ‘ä»¬ä¹‹å‰çˆ†ç‚¸å¾ˆæ­£å¸¸~~ï¼Œä½†æ˜¯å‰å‘å®šBNä¸å®šgradä¸ºå•¥æ²¡äº‹å‘¢...)
    * æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨è¾“å…¥çš„Max-Minæ¥ä»£æ›¿æ–¹å·®ï¼Œå†åŠ ä¸Šä¸€ä¸ªScaleAdjustTermï¼ˆå›ºå®šå€¼1/sqrt(2*ln(n))ï¼‰
      * è¯æ˜äº†åœ¨Gaussianæƒ…å†µä¸‹
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210172516.png)
    * Backwardåªæœ‰yçš„å¯¼æ•°æ˜¯ä½æ¯”ç‰¹çš„ï¼Œè€ŒWçš„æ˜¯å…¨ç²¾åº¦çš„
      * ï¼ˆä½œè€…argueè¯´åªæœ‰yçš„è®¡ç®—æ˜¯Sequentialçš„ï¼Œæ‰€ä»¥å¦å¤–ä¸€ä¸ªéƒ¨åˆ†ç»„ä»¶çš„è®¡ç®—ä¸è¦æ±‚å¾ˆå¿«ï¼Œæ‰€ä»¥å¯ä»¥å…¨ç²¾åº¦...ï¼‰
  * Part5 Theoretical Analysis
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210184548.png)
    * â­å¯¹å±‚çš„æ•æ„Ÿåº¦åˆ†ææœ‰å‚è€ƒä»·å€¼
  * GEMMLOWP
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210164913.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210165035.png)
      * æŒ‰ç…§chunkç¡®å®šï¼Œæ¥è§„é¿å¤§çš„Dynamic Range
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210165123.png)

* [Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss](http://openaccess.thecvf.com/content_CVPR_2019/papers/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.pdf)
	* Train a Quantize Interval, Orune & Clipping Together(è¿™tmä¸å°±æ˜¯clipå—ï¼Œclipåˆ°0å°±å«pruneå—ï¼Œå­¦åˆ°äº†)
	* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213191652.png)
	* å°†ä¸€ä¸ªQuantizeråˆ†ä¸ºä¸€ä¸ªTransformer(å°†å…ƒç´ æ˜ å°„åˆ°[-1,1]æˆ–è€…æ˜¯[0,1])å’Œä¸€ä¸ªDiscretezer
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213192019.png)
		* æˆ‘ç†è§£æ˜¯æŠŠQuantizeStepä¹Ÿå°±æ˜¯qDä½œä¸ºä¸€ä¸ªå‚æ•°(parameterize it)ï¼Ÿ
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213192826.png)
		* è¿™ä¸ªæ¼”ç¤ºåªæ¼”ç¤ºäº†æ­£åŠè½´ï¼Œè´ŸåŠè½´å¯¹ç§°
		* è¿™ä¸ªcxï¼Œdxä¹Ÿè¦å»å­¦ä¹ ?ï¼ˆå› ä¸ºä½œè€…æ–‡ä¸­å¹¶æ²¡æœ‰æåˆ°è¿™ä¸¤ä¸ªæ€ä¹ˆå–å¾—ï¼‰
		* ä½œè€…å¼ºè°ƒäº†æŠŠè¿™ä¸ªstepä¹Ÿå‚æ•°åŒ–ï¼Œç„¶åç›´æ¥ä»æœ€åçš„Losæ¥å­¦ä¹ è¿™äº›å‚æ•°ï¼Œè€Œä¸æ˜¯é€šè¿‡æ¥è¿‘å…¨ç²¾åº¦å‰¯æœ¬çš„L2èŒƒæ•°ï¼ˆè¿™ä¸ªæ€æƒ³å’ŒAnd The Bit Goes Downç±»ä¼¼ï¼‰ 
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213192259.png)
			* è™½ç„¶è¿™ä¸ªå˜æ¢è¡€å¤æ‚ï¼Œä½†æ˜¯ä½œè€…argueè¯´åªæ˜¯ä¸ºäº†inferenceï¼Œå‰å‘æ—¶å€™ç”¨çš„æ˜¯å›ºå®šè¿‡çš„å‚æ•°ï¼Œæ‰€ä»¥æ— æ‰€è°“
			* è¿™ä¸ªæ›´æ–°ç”¨åˆ°ä¹Ÿæ˜¯STEï¼Œä½†æ˜¯æˆ‘ç†è§£éƒ½è¿™ä¹ˆéçº¿æ€§äº†ï¼ŒSTEè¿˜èƒ½workå—ï¼Ÿ
		* Activationçš„Quantizerä¸å…¶ä¸åŒï¼Œé‚£ä¸ªæ¬¡æ–¹çš„å‚æ•°gammaæ˜¯1äº†
			* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213192512.png)


---

* [Mixed Precision Training of CNN using Interger]
  * DFP(Dynamic Fixed Point)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210194007.png)

---

* [Post Training 4 Bit Quantization of Conv Network For Rapid Deployment](https://arxiv.org/pdf/1810.05723.pdf)
  * Intel (AIPG)
  * No Need For Finetune / Whole Dataset
    * Privacy & Off-The-Shelf (Avoid Retraining) - Could Achieve 8 bit
  * 3 Methods
    * 1. Analutical Clipping For Integer Quantization
      * Analytical threshold for cliping Value
      * å‡è®¾QuantizationNoiseæ˜¯ä¸€ä¸ªå…³äºé«˜æ–¯æˆ–è€…[æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ](https://zh.wikipedia.org/wiki/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83)çš„å‡½æ•°
      * å¯¹ä¼ ç»Ÿçš„ï¼Œåœ¨min/maxä¹‹é—´å‡åŒ€é‡åŒ–çš„æƒ…å†µï¼Œroundåˆ°middle point
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213143844.png)
      * åˆ™MSEæ˜¯
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213143814.png)
      * Quantization Noise
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213144138.png)
      * Clipping Noise
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213144206.png)
      * Optimal Clipping 
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213144243.png)
        * **We Could Just Use This**
    * 2. Per Channel Bit Allocation
      * Overall MSE min
      * Regular Per Channel Quantization,æ¯ä¸€å±‚æœ‰ä¸€ä¸ªè‡ªå·±çš„Scaleå’Œoffsetï¼Œæœ¬æ–‡*ä¸åŒçš„Channelé€‰ç”¨äº†ä¸åŒçš„bitwidth*
      * åªè¦ä¿è¯æœ€åå¹³å‡æ¯å±‚è¿˜æ˜¯4bitï¼ˆæœ‰ç‚¹æµæ°“å“ˆï¼‰
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213144616.png)
      * è½¬åŒ–ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘æ€»å…±æœ‰Bä¸ªbitå¯ä»¥åˆ†é…ç»™Nä¸ªchanneï¼Œæˆ‘å¸Œæœ›æœ€åçš„MSEæœ€å°
      * æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213144836.png)
        * è®¡ç®—å‡ºæœ€ç»ˆçš„æœ€ä½³åˆ†é…ï¼Œç»™æ¯å±‚çš„Bi
          * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213144918.png)
      * *Assume That: Optimial Quantization Step Size is (Range)^(2/3)*
    * 3. After Quantization Will be a bias in mean/var
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213145006.png)
  * Related Works
    * ACIQæ˜¯è¿™ç¯‡æ–‡ç« çš„æ–¹æ³•ï¼Œæ›´æ—© (Propose Activation Clip post training)
      * æ›´æ—©æœ‰äººç”¨KLæ•£åº¦å»æ‰¾Clipping Valueï¼ˆéœ€è¦ç”¨å…¨ç²¾åº¦æ¨¡å‹å»è®­ç»ƒï¼ŒèŠ±æ—¶é—´ï¼‰
      * æœ¬è´¨éƒ½æ˜¯å»Handle Statistical Outlier
        * æœ‰äººåˆ†è§£Channel 
        * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213143501.png)
          * ç»†èŠ‚æ²¡æ¸…æ¥šï¼Œæ”¾è¿™è¾¹çœ‹ä¹‹åæœ‰å’©æœ‰æ—¶é—´çœ‹

* [Accurate & Efficient 2 bit QNN](cn.bing.com/?toHttps=1&redig=7C8B48CACF7748ACB6C926F9E0DBECE4)
  * PACT + SAWB
    * SAWB(Statistical Aware Weight Bining)-ç›®çš„æ˜¯ä¸ºäº†æœ‰æ•ˆçš„åˆ©ç”¨åˆ†å¸ƒçš„ç»Ÿè®¡ä¿¡æ¯(å…¶å®ä¹Ÿå°±æ˜¯ä¸€äºŒé˜¶ç»Ÿè®¡é‡)
    * ä¼˜åŒ–çš„ç›®æ ‡è¿˜æ˜¯æœ€å°åŒ–é‡åŒ–è¯¯å·®(æ–°weightå’ŒåŸweightçš„L2èŒƒæ•°)
    * è¯´ä¹‹å‰å–Scaleç”¨æ‰€æœ‰å‚æ•°çš„meançš„æ–¹å¼åªæœ‰åœ¨åˆ†å¸ƒNormalçš„æ—¶å€™æ‰æˆç«‹
    *  ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213150637.png)
    * å‚æ•°C1ï¼ŒC2æ˜¯çº¿æ€§æ‹Ÿåˆå‡ºæ¥çš„ï¼Œæ ¹æ®bitwidth
      * é€‰å–äº†å‡ ç§æ¯”è¾ƒå¸¸è§çš„åˆ†å¸ƒ(Gauss,Uniform,Laplace,Logistic,Triangle,von Mises)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213150944.png)
      * (æˆ‘ç†è§£è¿™ä¸ªå¥½åƒåªæ˜¯åœ¨è¯æ˜ä¸€é˜¶å’ŒäºŒé˜¶ç»Ÿè®¡é‡å°±è¶³å¤Ÿæ‹Ÿåˆäº†)å›¾é‡Œé¢çš„ç‚¹æ˜¯æ¯ä¸ªåˆ†å¸ƒçš„æœ€ä½³Scale
        * ä¸Šé¢çš„å®éªŒæ˜¯å¯¹åº”ä¸€ç§é‡åŒ–é—´éš”ï¼Œä½œè€…åé¢çš„å®éªŒåˆè¯´æ˜å¯¹äºå¤šä¸ªé‡åŒ–é—´éš”åŒç†
        * æ‰€ä»¥æœ€å¤§çš„è´¡çŒ®å…¶å®æ˜¯**å¯¹äºå®é™…çš„åˆ†å¸ƒï¼Œç”¨å®é™…åˆ†å¸ƒçš„ç»Ÿè®¡é‡å»æ‰¾è¿™ä¸ªæœ€ä½³çš„Scaling**
  * æ–‡ä¸­åˆ†æäº†PACTå’ŒReluä¸€æ ·æœ‰è¡¨è¾¾èƒ½åŠ›
  * è¿˜ç»™äº†ä¸€ä¸ªSystemDeisgnçš„Insght

---

* [TTQ]()
  * Han
  * DoReFa(Layer-Wise Scaling Factor:L1Norm)
  * TWN(Ternary Weight Net)(View As Optmize Problem Of Minimizing L2 Norm Between)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210130401.png)
    * tæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå¯¹æ‰€æœ‰å±‚ä¿æŒä¸€è‡´
  * TWNçš„æ­¥éª¤èå…¥äº†Quantize-Aware Training
  * Scaling Factor
    * DoReFaç›´æ¥å–L1 Normçš„mean
    * TWNå¯¹fp32çš„Weightï¼Œæœ€å°åŒ–L2èŒƒæ•°ï¼ˆXnorä¹Ÿæ˜¯ï¼‰
    * TTQè¿™é‡Œæ˜¯è®­ç»ƒå‡ºæ¥çš„ï¼ˆæ‰€ä»¥å¹¶ä¸æ˜¯æ¥è‡ªæ•´ä¸ªå‚æ•°çš„åˆ†å¸ƒï¼Œè€Œæ˜¯ç‹¬ç«‹çš„å‚æ•°ï¼‰
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191210132214.png)

---

* [TernGrad](https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf)
  * æ•°å­¦è¯æ˜äº†æ”¶æ•›(å‡è®¾æ˜¯æ¢¯åº¦æœ‰ç•Œ)
  * æ˜¯From Scratchçš„
  * åšäº†Layer-Wiseçš„Ternary/ä»¥åŠGradient Clipping
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213172324.png) 
    * å…¶ä¸­btæ˜¯ä¸€ä¸ªRandom Binary Vector
    * Stochastic Rounding
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213172521.png)
  * Scale Sharing
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213172635.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213172728.png)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213172838.png)
  * ä»–èƒ½workæˆ‘ä»¬ä¸èƒ½workï¼Ÿï¼ˆå‰å‘æ˜¯å…¨ç²¾åº¦çš„ï¼Ÿï¼‰
 

---


* æ—©æœŸçš„ä¸€äº›äºŒå€¼åŒ–ç½‘ç»œçš„å»¶ç”³
  * XnorNetæ–‡ç« æ˜¯åŒæ—¶æäº†BinaryWeightedNetworkå’ŒXNORNet
    * å‘é‡ä¹˜å˜ä¸ºäºŒå€¼å‘é‡åšbitcount
    * åå‘ä¼ æ’­çš„æ—¶å€™ä¹Ÿå¯ä»¥æŠŠæ¢¯åº¦ç¼–ç¨‹Ternaryï¼Œä½†æ˜¯éœ€è¦å–Maxä½œä¸ºScalingFactorè€Œä¸æ˜¯æœ€å¤§å€¼
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213153040.png)
      * å¾ˆç¥å¥‡ 
      * [Source Code](https://github.com/allenai/XNOR-Net/blob/master/models/alexnetxnor.lua) 
      * åˆ«äººçš„ä¸€ä¸ªå¤ç°![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213154204.png)
      * å‚è€ƒäº†[è¿™ä¸ªissue](https://github.com/allenai/XNOR-Net/issues/4)æ˜¯å‚ä¸å‰å‘è¿ç®—çš„
  * **æ³¨æ„åªæœ‰XNORNetæ˜¯inputå’Œweightå…¨éƒ¨éƒ½æ˜¯äºŒå€¼ï¼Œæ‰€ä»¥å¯ä»¥ç”¨Bitcountçš„è¿ç®—ï¼è€Œåç»­çš„TWNå’ŒTTQéƒ½æ²¡æœ‰æè¿™ä¸ªäº‹æƒ…ï¼Œè€Œä»–ä»¬æ˜¯åªå¯¹Weightåšäº†ä¸‰å€¼ï¼**
  * åé¢æ˜¯TernaryNetï¼ˆTWNï¼‰
    * äºŒå€¼å˜ä¸‰å€¼
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213163328.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213163424.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213163501.png)
  * ABCNet (Accurate Binary Conv)
    * ç”¨å¤šä¸ªBinaryçš„LinearCombinatoinæ¥ä»£è¡¨å…¨ç²¾åº¦weight
  * DoReFa
    * ä¹Ÿå¯ä»¥åŠ é€Ÿåå‘æ¢¯åº¦
    * å’Œxnorçš„åŒºåˆ«æ˜¯ï¼Œxnorçš„å–saclefactoræ˜¯é€channelçš„è€Œå®ƒæ˜¯é€layerçš„
      * ~~Dorefaä½œè€…è¯´xnorçš„æ–¹å¼åšä¸äº†backpropåŠ é€Ÿï¼Œä½†æ˜¯xnorä½œè€…åœ¨åŸæ–‡ä¸­è¯´ä»–ä¹Ÿbinarizeäº†æ¢¯åº¦~~
  * TTQ ï¼ˆæˆ‘å…¨éƒ½è®­ï¼‰
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213162015.png)
    * Backprop
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213162414.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213164311.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213162446.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213165007.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191213162700.png)

---

### [AMC-AutoML For Model Compression and Accleration on Mobile Devices]
* ECCV 2018
* Replace Handcrafted heuristics & rule-based Policy via AutoML
* Also Perform Better

---

* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214170306.png)
	* Process Pretrained Model Layer-By-Layer
	* RL Agent receive Embedding Of an Layer,outtput prune ratio at
	* å¼ºè°ƒäº†ç½‘ç»œlayerså¹¶ä¸æ˜¯independentçš„
	* Rule-Based couldn't Transfer(æ— æ³•åº”å¯¹æ–°çš„æ¶æ„)
		* Deeper Net, More Search Space
	* å¯¹äºæ¯ä¸€ä¸ªLayerï¼Œåšä¸€ä¸ªEncodingï¼Œä¹‹åï¼Œå–‚ç»™RL Agentï¼Œå…¶è¾“å‡ºä¸€ä¸ªPruneRatioï¼Œå‰ªå®Œä¹‹åå†ç»™åˆ°ä¸‹ä¸€å±‚
* ä¸¤ç§æ–¹å¼
	* Resource-Constraint: é™åˆ¶èµ„æºFlopsï¼Œåªå»å°è¯•é‡‡æ ·å‡ºæ¥çš„é‚£äº›æ¨¡å—
		* action space(prune ratio) è¢«é™åˆ¶ä¸ºäº†åªæœ‰ä½äºresourceçš„æ‰ä¼šè¢«é‡‡æ · 
	* Acc-Constraint : 
* åœ¨åŒ…æ‹¬Classificationå’ŒDetcetionçš„å¤šä¸ªåœ°æ–¹åšäº†å®éªŒ
* **æœ¬è´¨ä¸Šæ˜¯ç”¨ä¸€ç§éå¸¸é»‘ç›’çš„æ–¹å¼ç›´æ¥ä»ä¸€ä¸ªpretrainæ¨¡å‹ä¸­å­¦ä¹ å‡ºä¸€ä¸ªAgentæ¥ç»™å‡ºæ¯ä¸€å±‚çš„PruneRatio**
  * å¯¹äºResource-Constraintçš„åœºæ™¯ï¼Œé‡‡ç”¨äº†ç›´æ¥å°†æœç´¢ç©ºé—´åŠ ä¸€ä¸ªé™åˆ¶çš„æ–¹å¼æ¥åš
  * è¿™ç§æ–¹å¼ç†è®ºä¸Šå¾ˆéš¾è®­ç»ƒå‡ºæ¥ï¼Œä½†æ˜¯å…¶å®éªŒç»“æœè¯æ˜äº†å¯ä»¥æœå‡ºä¸€ä¸ªæ¶æ„ï¼ˆä½†æ˜¯éœ€è¦æœå¤šä¹…å‘¢ï¼Ÿï¼‰è¿™ä¸ªAgentæ˜¯é’ˆå¯¹æŸä¸€ä¸ªç½‘ç»œçš„ï¼ˆç­‰ä¼šå®ƒçš„weightéœ€è¦ç¡®å®šå—ï¼Ÿï¼‰
  * *æˆ‘ä»¬æ˜¯å¦å¯ä»¥è¯´æ˜å¯¹AutoPrunningè¿™ä¸ªé—®é¢˜å…¶å®æ²¡æœ‰é‚£ä¹ˆå¤æ‚ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªè¿‘ä¼¼çº¿æ€§çš„æ¨¡å‹ç›´æ¥æ‹Ÿåˆå‡ºä»¥ä¸€ä¸ªèƒ½å¤Ÿå­¦ä¸‹å»çš„æ¨¡å‹*  
    * è¿™æ ·çš„ä¸€ä¸ªé—®é¢˜å°±æ˜¯ä¸èƒ½ä¿è¯æ³›åŒ–èƒ½åŠ›ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„æ‹Ÿåˆç³»æ•°ä¸èƒ½æ³›åŒ–è¿‡å»ï¼‰è¿™ä¸ªæ˜¯å¦å¯ä»¥é€šè¿‡å®éªŒè¯æ˜ï¼Œå…¶å®è¿™ä¸ªé—®é¢˜æ²¡æœ‰è¿™ä¹ˆå¤æ‚ï¼Ÿ
  * å®ƒåˆ©ç”¨RL Agentå¯¹æ¯ä¸€å±‚çš„æ•æ„Ÿåº¦è¿›è¡Œäº†ä¸€ä¸ªé»‘ç›’çš„å»ºæ¨¡
    * ä½†æ˜¯æˆ‘ä»¬çš„æ‰€è°“Fitçš„æ¨¡å—å°±æ˜¯æ˜¾å¼çš„å»æ‰¾è¿™ä¸ªæ•æ„Ÿåº¦
    * *ç­‰ä¼šæˆ‘ä»¬è¯´çš„é‚£ä¸ªå‡è®¾æ˜¯å„å±‚ä¹‹é—´éœ€è¦æ—¶ç‹¬ç«‹çš„ï¼ŒAMCçš„èƒŒæ™¯é‡Œæäº†æ¯ä¸€å±‚å¯èƒ½æ—¶ä¸ç‹¬ç«‹ï¼Œå®ƒçš„å±‚é—´å»ºæ¨¡æ–¹å¼æœ‰ä¸€äº›è¯¡å¼‚*
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214181936.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214182333.png)
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214182533.png)
        * å¥½åƒå°±åªæ˜¯å¾ˆæ•·è¡çš„è¡¨ç¤ºè¿™ä¸ªå±‚é—´ä¿¡æ¯æˆ‘ä»¬èå…¥è¿›æ¥äº†
* æˆ‘è§‰å¾—Floorè¿™ä¸ªç‚¹éå¸¸æœ‰æ„ä¹‰ï¼Œç”šè‡³å¯ä»¥å•ç‹¬é¢†å‡ºæ¥æ€è€ƒï¼Œå› ä¸ºä¸€äº›é»‘ç›’å­ï¼Œæ˜¯å¾ˆéš¾æ˜¾å¼çš„å»ºæ¨¡å‡ºä¸€äº›å¾ˆæ˜æ˜¾çš„å…ˆéªŒçš„ï¼Œæ¯”å¦‚è¯´16~20   
  * è€ŒAmcçš„Agentè¾“å…¥Embeddingä¸­æ˜¯åŒ…å«äº†Flopsä¿¡æ¯çš„ï¼Œä¼šæœ‰ä¸€äº›éå¸¸explicitçš„ruleï¼Œ**é»‘ç›’å­å»ºæ¨¡å¾—ä¸å¿å¤±**
* Fine-Grained Action Space,å¦‚æœæ˜¯discrete actionçš„è¯ï¼Œå¾ˆéš¾exploreå‡ºæ¥ï¼ŒåŒæ—¶å¿½ç•¥äº†orderä¿¡æ¯ï¼Œæ‰€ä»¥ç”¨DDPGï¼Œæ¥åœ¨è¿ç»­åŸŸåšæ‹Ÿåˆ
* DDPG Agentæ¥æ”¶åˆ°çš„æ˜¯ä¸€ä¸ªStreamOfLayerï¼Œæ•´ä¸ªç½‘ç»œè¿‡å®Œäº†ä¹‹åæ”¶åˆ°ä¸€ä¸ªreward(ä½¿ç”¨çš„Accæ˜¯finetuneä¹‹å‰çš„)
    * é™å®šaction spaceï¼Œç›´æ¥å°†Rewardè®¾ç½®ä¸º-Error
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214191907.png)
      * è¿™æ ·æš´åŠ›çš„æ–¹æ³•å¯¹å®é™…ç¡¬ä»¶è®¾è®¡ï¼ŒFlopså’ŒPruneRatioä¸¥æ ¼æ­£ç›¸å…³ï¼Œä½†æ˜¯ç¡¬ä»¶å…¶å®ä¸ä¸€å®šï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæ˜ç¡®çš„äº‹æƒ…ï¼Œå¯ä»¥ç›´æ¥åŠ ä¸Šï¼ˆï¼Ÿï¼‰
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214193113.png)
  * åšäº†è¿™æ ·çš„ä¸€ä¸ªå‡è®¾ï¼Œæˆ‘æ„Ÿè§‰è¿˜æ˜¯æŒºæš´åŠ›çš„ï¼Œå›å¤´åº”è¯¥çœ‹ä¸€ä¸‹è¿™ä¸ªç»“è®ºæ˜¯å“ªé‡Œæ¥çš„
* è®¾ç½®çš„æ—¶å€™å¹¶æ²¡æœ‰æŠŠBNåŠ å…¥åˆ°Convå±‚ä¸­
* ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214195522.png)
  * æ„Ÿè§‰ä¹Ÿæ²¡æœ‰å·®å¤ªå¤šå•Šï¼Œå› ä¸ºå¤§å®¶éƒ½è¦Finetuneçš„å˜›
  
---

* æœ‰ä¸€ä¸ªå°é—®é¢˜ï¼Œå°±æ˜¯æˆ‘ä»¬çš„æ•æ„Ÿåº¦åˆ†æï¼Œçš„ç¡®æ²¡æœ‰åŠæ³•å»ºæ¨¡å‡ºå¤šå±‚ä¹‹é—´çš„å½±å“ï¼Œè¿™ä¸ªè¦ä¹ˆè¦è¯´æ˜å¯ä»¥çœ‹ä½œç‹¬ç«‹ï¼Œè¦ä¹ˆè¦åŠ å±‚é—´
  * ç°åœ¨å°±æ˜¯å¯¹æ¯ä¸€å±‚å•ç‹¬è€ƒè™‘ï¼Œä½†æ˜¯æˆ‘è§‰å¾—æˆ‘ä»¬å¦‚æœæ­¥é•¿æ¯”è¾ƒå°çš„è¯é—®é¢˜ä¸å¤§
* æˆ‘æ„Ÿè§‰éœ€è¦é—®ä¸€ä¸‹æ¶›å“¥å…³äºDDPGçš„setbackä»¥åŠå®ƒè¿™ç§ç›´æ¥ç æœç´¢ç©ºé—´çš„æ–¹æ³•åˆä¸åˆç†
* AMCçš„å»ºæ¨¡æ˜¯å¯¹ç½‘ç»œç»“æ„çš„å»ºæ¨¡ï¼Œå¦‚æœweightå‘ç”Ÿæ”¹å˜æ˜¯ä¸æ˜¯è¿™ä¸ªå°±ä¸æ˜¯å¾ˆæˆç«‹äº†ï¼Ÿéœ€è¦é‡æ–°å†è®­ç»ƒä¸€ä¸ªagent
  * agentæ˜¯ä¸æ˜¯weight-invariantçš„
* [AutoPruner: An End-to-End Trainable Filter Pruning Method forEfficient Deep Model Inference](https://arxiv.org/pdf/1805.08941.pdf)
  * åœ¨Finetuneçš„è¿‡ç¨‹å½“ä¸­å®ŒæˆPrun
* [ECC: Platform-Independent Energy-Constrained Deep Neural Network Compression via a Bilinear Regression Mode](https://arxiv.org/pdf/1812.01803.pdf)
  * æœ‰ä¸€äº›ç±»ä¼¼ï¼Œä¸è¿‡ä»–ä»¬Regressiondçš„æ˜¯Energyï¼Œè€Œä¸”æœ€åå®éªŒæ˜¯åœ¨TXä¸Šåšçš„
  * æ˜¯æ•´ä¸ªæ¨¡å‹å‹ç¼©ï¼Œè€Œä¸æ˜¯layer-by-layer
  * relatedworkåˆ—ä¸¾äº†å‡ ä¸ªå·¥ä½œæ¯”è¾ƒæœ‰ä»·å€¼
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20191214233728.png)
* [Automated Pruning for Deep Neural Network Compression](https://ieeexplore.ieee.org/abstract/document/8546129)
  * å¯å¯¼çš„AutoPrune
* [AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates](https://www.researchgate.net/profile/Zhiyuan_Xu9/publication/334316382_AutoSlim_An_Automatic_DNN_Structured_Pruning_Framework_for_Ultra-High_Compression_Rates/links/5ddf9aab4585159aa44f1634/AutoSlim-An-Automatic-DNN-Structured-Pruning-Framework-for-Ultra-High-Compression-Rates.pdf)
  * æå‡ºäº†ä¸€ä¸ªè¶…çº§ç‰›é€¼çš„Automated Framework **AutoCompress**
  * ç”¨äº†Advanced Pruningæ–¹æ³•**ADMM**

---

* [Layer Compensated Prunning For Resource Constraint CNN](https://arxiv.org/pdf/1810.00518.pdf)
  * ä¹‹å‰å¤§å¤šéƒ½æ˜¯ç¡®å®šæ¯å±‚éœ€è¦å‰ªå¤šå°‘ï¼Œç„¶ååœ¨å±‚å†…æ’åºï¼›è¿™é‡Œçœ‹æˆä¸€ä¸ªGlobal Sortingçš„é—®é¢˜
  * ç”¨äº†Meta Learningï¼Œæ‰¾åˆ°äº†æœ€å¥½çš„solutionï¼Ÿ
* [Efficient Neural Network Compression](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Efficient_Neural_Network_Compression_CVPR_2019_paper.pdf)
  * å¿«é€Ÿçš„æ‰¾SVDåˆ†è§£å¯¹åº”çš„é‚£ä¸ªRank
  * ~~æŠŠä»–æŒ‚åœ¨è¿™é‡Œåªæ˜¯å› ä¸ºè¿™ä¸ªé¢˜ç›®å®åœ¨æœ‰ç‚¹éœ¸æ°”...~~
  * [AutoRank: Automated Rank Selection for Effective Neural Network Customization](https://mlforsystems.org/assets/papers/isca2019/MLforSystems2019_Mohammad_Samragh.pdf)
* [Leveraging Filter Correlation For Deep Model Compression](https://arxiv.org/abs/1811.10559)
  * å¯ä»¥è®¤ä¸ºä»–ä»¬è®¤ä¸ºFilteræ˜¯æ€ä¹ˆç›¸å…³çš„æ¥æ”¯æŒæˆ‘ä»¬çš„ç‹¬ç«‹æ€§å‡è®¾
* [SNN Compression](https://arxiv.org/abs/1911.00822)
* [PruneTrain: fast neural network training by dynamic sparse model reconfiguration](https://dl.acm.org/citation.cfm?id=3356156)
  * Lassoçš„é‚£ç¯‡æ–‡ç« 
* [EPNAS: Efficient Progressive Neural Architecture Search](https://arxiv.org/abs/1907.04648)
* [Mapping Neural Networks to FPGA-Based IoT Devices for Ultra-Low Latency Processing](https://www.mdpi.com/1424-8220/19/13/2981)
  * ä¸æ˜¯9102å¹´è¿˜å‘è¿™ç§æ–‡ç« 
* [SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks](https://arxiv.org/abs/1812.08301)
  * å°è¯•ä¸€æ­¥åˆ°ä½ï¼Œä½†æ˜¯å¥½åƒæ²¡æœ‰å¼•ç”¨
* [Band-limited Training and Inference for Convolutional Neural Networks](http://proceedings.mlr.press/v97/dziedzic19a.html)
* [PocketFlow: An Automated Framework for Compressing and Accelerating Deep Neural Networks](https://openreview.net/forum?id=H1fWoYhdim)
  * ä¹Ÿæ˜¯automationï¼Œä½†æ˜¯ä¸»è¦è´¡çŒ®ç‚¹å’Œæˆ‘ä»¬ä¸æ˜¯ç‰¹åˆ«ä¸€æ ·ï¼ˆTecent NIPS2018ï¼‰
* [Cascaded Projection: End-To-End Network Compression and Acceleration](http://openaccess.thecvf.com/content_CVPR_2019/papers/Minnehan_Cascaded_Projection_End-To-End_Network_Compression_and_Acceleration_CVPR_2019_paper.pdf)
  * CVPR2019 Low-Ranlk Projection
* [Low Precision Constant Parameter CNN on FPGA](https://arxiv.org/pdf/1901.04969.pdf)
  * çº¯FPGAçš„æ–‡ç« ï¼Œæ‘˜è¦å¾ˆçŸ­ï¼Œå¾ˆå¥½å¥‡
* [Reconstruction Error Aware Pruning for Accelerating Neural Networks](https://link.springer.com/chapter/10.1007/978-3-030-33720-9_5)
  * è¿™ä¸ªæ‰€è°“çš„reconstruct erroræ˜¯å•¥ï¼Œæ˜¯ä¸æ˜¯å¯ä»¥èå…¥åˆ°æˆ‘ä»¬çš„æ•æ„Ÿåº¦åˆ†æä¹‹ä¸­ï¼Ÿ 
* [ReForm: Static and Dynamic Resource-Aware DNN Reconfiguration Framework for Mobile Device](https://dl.acm.org/citation.cfm?id=3324696)
  * DAC19
* [Network Pruning via Transformable Architecture Search](https://arxiv.org/abs/1905.09717)
* [AutoML for Architecting Efficient and Specialized Neural Networks](https://ieeexplore.ieee.org/abstract/document/8897011)
  * æœ‰ç‚¹ç‰›é€¼ï¼Œè¯´æœ‰autoprunningå’Œautomixedprecision
* [BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search](https://arxiv.org/abs/1910.11858)
* [Accelerate CNN via Recursive Bayesian Pruning](http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.html)
* [A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection](https://ieeexplore.ieee.org/abstract/document/8916629/)
* [Neural Network Pruning with Residual-Connections and Limited-Data](https://arxiv.org/abs/1911.08114)
* [AutoQB: AutoML for Network Quantization and Binarization on Mobile Devices](https://arxiv.org/abs/1902.05690)
* [PruneTrain: Gradual Structured Pruning from Scratch for Faster Neural Network Training](https://arxiv.org/abs/1901.09290)
