# åˆè¡·

* ï¼ˆè¡¨æ˜è¯¥æ•´ç†çš„ç›®çš„ï¼Œä¸ºåç»­å·¥ä½œå±•å¼€åšä¸€äº›æŒ‡å¼•ï¼‰
*  æœ¬æ–‡ä¸å®Œå…¨åŒ…æ‹¬äº†ä¸€äº›NN Fixed-Point Trainingçš„å¯èƒ½å®ç°æ–¹å‘ï¼Œä¸ºåç»­å®éªŒæŒ‡è·¯
*  æœ¬æ–‡çš„æœ€ç»ˆç›®çš„æ˜¯é¢å‘FPGAç­‰åµŒå…¥å¼è®¾å¤‡çš„NN éƒ¨ç½²/è®­ç»ƒï¼Œçš„ä¸€ä¸ªè½¯ä»¶ä»¿çœŸå·¥å…·ã€‚ä¸ºäº†Scalability,ä¼šå°è¯•å®ç°ä¸€äº›è¾ƒä¸ºGeneralçš„å®šç‚¹è®­ç»ƒæŠ€å·§
   *  æ•…å¯èƒ½**ä¸ä¼šåŒ…å«**BinaryNetworkç­‰è¾ƒä¸º*æ¿€è¿›/ä¸“é—¨åŒ–(éœ€è¦é¢å¤–ç¡¬ä»¶è®¡ç®—ç»“æ„è®¾è®¡)*çš„å®ç°æ–¹å¼
   *  ä»¥åŠè¯¸å¦‚MobileNet, ShuffleNet ç­‰è½»é‡åŒ–ç½‘ç»œè®¾è®¡
*  Quantizationçš„ä¸¤ç§ä¸»è¦æ–¹å¼
   *  åŸºäºCodeBookçš„(Deep Compression) ï¼šå®é™…å‚æ•°è¿˜æ˜¯é«˜ç²¾åº¦çš„ï¼Œæ— æ³•åˆ©ç”¨å®šç‚¹è®¡ç®—è¿›è¡ŒåŠ é€Ÿï¼Œä»…èƒ½å‡å°‘å­˜å‚¨
   *  åŸºäºå®šç‚¹æ•°(Fixed Pointè¡¨ç¤º)ï¼Œ(IBMçš„FP8ä¹Ÿå¯ä»¥å½’å…¥æ­¤ç±») ï¼š å¯åˆ©ç”¨å®šç‚¹è®¡ç®—åŠ é€Ÿï¼Œæ•…æœ¬æ–‡ä¸»è¦é‡‡å–è¯¥æ–¹å¼
*  ç›®å‰é¢„è®¡çš„å‡ ç§åœºæ™¯
   *  Post-Training Quantization : åœ¨å®ŒæˆFPè®­ç»ƒä¹‹åçš„å‹ç¼©ï¼Œäº§ç”Ÿå®šç‚¹çš„W/Aè¿›è¡Œéƒ¨ç½²
      *  Exampleï¼šDistillationï¼ŒEntropyConstraintQï¼Œ IncrementalQ
   *  Quantize-Aware Training ï¼š åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è€ƒè™‘å®šç‚¹çš„å½±å“ï¼Œåœ¨è®­ç»ƒä¸­é‡‡å–Fixed,äº§ç”Ÿå®šç‚¹çš„W/Aè¿›è¡Œéƒ¨ç½²
      *  Exampleï¼š StraightThroughActivationçš„æ–¹æ³•ï¼ˆè®­ç»ƒæ—¶ç”¨Fixed Inferenceï¼Œä½†æ˜¯æ¢¯åº¦æ˜¯å¯¹åº”çš„å…¨ç²¾åº¦å‰¯æœ¬åšï¼‰
   *  Fixed-Point Training: è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œ**çº¯å®šç‚¹(W/G/A)**ï¼Œæ¨¡æ‹Ÿåœ¨çº¯å®šç‚¹è®¾å¤‡ä¸Šè¿›è¡Œè®­ç»ƒ
      *  Exampleï¼šWAGE
      *  ~~æœ‰ç‚¹æ¿€è¿›ï¼Œä¸çŸ¥é“æ˜¯å¦èƒ½å®ç°~~

# Methods

> ä»è‡ªå·±çš„å‡ºå‘ç‚¹å¯¹çœ‹åˆ°çš„ä¸€äº›å®šç‚¹ç›¸å…³å·¥ä½œçš„æ–¹æ³•ä¸æ€æƒ³çš„çº¯ä¸»è§‚å½’çº³ï¼ˆå¯èƒ½å­˜åœ¨åå·®ç”šè‡³é”™è¯¯ï¼‰

> è¯¥ç§åˆ’åˆ†æ–¹å¼æ²¡æœ‰ä»€ä¹ˆé“ç†ï¼Œåªæ˜¯æˆ‘å¼ºè¡ŒåŒºåˆ†çš„ç½¢äº†ï¼‰

## A. Post-Training Quantization

> è¯¥ç±»æ–¹æ³•æœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯åˆ©ç”¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå‹ç¼©ï¼Œä¸Quantized-Aware Trainingç›¸å¯¹æ¯”æœ‰å‡ åˆ† 2-Stageçš„æ„å‘³ï¼Œä¼˜åŠ¿åœ¨äºå¯ä»¥åˆ©ç”¨å·²ç»åŸºæœ¬ç¡®å®šçš„å‚æ•°åˆ†å¸ƒå»åˆ†æï¼Œé‡‡å–é‡åŒ–ç­–ç•¥,å¯¹å¤§æ¨¡å‹æ¯”è¾ƒæœ‰æ•ˆï¼Œä½†æ˜¯å°æ¨¡å‹ä¼šå´©

* [1510-Deep Compression](https://arxiv.org/abs/1510.00149)
  * åˆ©ç”¨äº†å‚æ•°åˆ†å¸ƒçš„çŸ¥è¯†ï¼Œé‡‡ç”¨K-Means
* [1702-Incremental Quantization](https://arxiv.org/abs/1702.03044)
  * åˆ†ç»„-é‡åŒ–-Finetuneçš„æµç¨‹ï¼Œæ¯æ¬¡retrainåªæœ‰å½“å‰ç»„è¢«é‡åŒ–ï¼Œè¿­ä»£ç›´åˆ°æ‰€æœ‰å‚æ•°éƒ½è¢«é‡åŒ–ï¼ˆä»‹äºAä¸Bä¹‹é—´ï¼‰
* [1802-Model compression via distillation and quantization](https://arxiv.org/abs/1802.05668)
  * ICLR 2018
  * åˆ©ç”¨ä¸€ä¸ªå¾ˆå¤§çš„Tecaheræ¥å¼•å¯¼ä½æ¯”ç‰¹ç½‘ç»œ
* [1810-Post training 4-bit quantization of convolutional networks for rapid-deployment(NIPS 2019)](https://arxiv.org/abs/1810.05723)
    * Intel, No Need To Finetune On Full Dataset
    * 3 Methods,ä¸€äº›ç†è®ºæ¨å¯¼å¾—åˆ°æ¯channelæ¯”ç‰¹æ•°çš„åˆ†é…ä»¥åŠClippingValueå¯ä»¥ä¾æ®å·²æœ‰çš„å‚æ•°åˆ†å¸ƒç®—å‡ºæ¥
* [1907-And the Bit Goes Down: Revisiting the Quantization of Neural Networks](https://arxiv.org/abs/1907.05686)

* Other Methods    (~~More Of a Digest Not Important~~)
  * [1511-Fixed Point Quantization of Deep Convolutional Network](https://arxiv.org/abs/1511.06393)
     * é«˜é€š åŸºäºSQNR,å‰Deep Compressionçš„ä¸Šå¤æ—¶æœŸï¼Œæ²¡æœ‰ä»€ä¹ˆå¤§çš„äº®ç‚¹
  * [Entropy Constraint Scalar Quantization](https://www.mdpi.com/1099-4300/18/12/449)
    * å¯¹æ¯ä¸€ä¸ªå‚æ•°çš„Gradientåšæ³°å‹’å±•å¼€å¹¶ä¸”èˆå¼ƒé«˜é˜¶é¡¹ï¼ŒåŒ–ç®€å¾—åˆ°å“ªäº›å‚æ•°å¯¹æœ€ç»ˆLossé‡è¦ï¼Œä»¥æ­¤ä½œä¸ºå‰ªææˆ–è€…é‡åŒ–çš„ä¾æ®(é€‰å–èšç±»ä¸­å¿ƒ)
    * ~~ä¹Ÿå¯ä»¥ä½œä¸ºå‰ªæçš„ä¾æ®~~
    * å’Œå‰ªæçš„è¿™ç¯‡ï¼Œæœ‰ä¸€å®šç›¸å…³æ€§[1611-Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning](https://arxiv.org/abs/1611.05128)
      * è´ªå¿ƒçš„å‰ªå»å¯¹æœ€åLosså½±å“ä¸å¤§çš„ (ä¹Ÿå°±æ˜¯TCP(Transfer Channel Prunning)ä¸­çš„å‰ªææ–¹å¼)
      * [1711-NISP: Pruning Networks using Neuron Importance Score Propagation](https://arxiv.org/abs/1711.05908)ä¹Ÿä¼šæ¶‰åŠ
  * [1805-Retraining-Based Iterative Weight Quantization for Deep Neural Networks](https://arxiv.org/abs/1805.11233)
  * [1906-Data-Free Quantization through weiht equailization & Bias Correction](https://arxiv.org/abs/1906.04721)

## B. Quantize-Aware-Training

> ç›¸æ¯”äºç¬¬ä¸€ç±»ï¼Œè¯¥ç±»æ–¹æ³•çš„ä¸»è¦ä¼˜åŠ¿åœ¨äº1-Stageï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹

* æ—©æœŸçš„ä¸€äº›Binary/XNORNetå‡å±äºæ­¤ç±»ï¼Œå¤§éƒ¨åˆ†åŸºäºStraightThroughActivationçš„æ€æƒ³ï¼Œå³è®¤ä¸ºå®šç‚¹è¿‡ç¨‹çš„å¯¼æ•°ä¸º1
  * [1603-XNORNet](https://arxiv.org/abs/1603.05279)
    * å¯¹WAäºŒå€¼åŒ–ï¼ŒåŠ ä¸Šä¸€ä¸ªL1Normçš„meanä½œä¸ºæ¯å±‚çš„ScalingFactorï¼ˆå…¶æ”¹è¿›DoReFaåŠ ä¸Šäº†Gçš„ï¼‰
  * [1605-TWN](https://arxiv.org/abs/1605.04711)
    * TWN - æœ€å°åŒ–å…¨ç²¾åº¦weightä¸Ternary Weightä¹‹é—´çš„L2 Norm 
  * [1709-WRPN-Intel-ICLR2018](https://arxiv.org/abs/1709.01134)
    * ä½æ¯”ç‰¹WA(å…¨ç²¾åº¦G)ï¼Œä½†æ˜¯è®©ç½‘ç»œæ›´wide(å¢å¤šäº†FeatureMapæ•°é‡)
* [1712-Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877)
  * åŒ…å«äº†ä¸€ä¸ªæµ®ç‚¹çš„Scale Factor,æœ€å¤§åŒ–åˆ©ç”¨äº†åŠ¨æ€èŒƒå›´ï¼Œåˆ©ç”¨äº†(0.45-0.5)çš„è¿™ä¸€æ®µç©ºé—´
  * éå¯¹ç§°é‡åŒ–(æœ‰ä¸€ä¸ªé›¶ç‚¹ï¼Œä»¥åŠä¸€ä¸ªæµ®ç‚¹çš„Scale - é€å±‚)
  * Merge Conv-BN
* [1805-PACT](https://arxiv.org/abs/1805.06085)
  * è®­ç»ƒä¸­Quantize Activationï¼Œè®­ç»ƒä¸€ä¸ªactivation clipping parameter(ä¿®æ”¹Reluçš„clipèŒƒå›´s)(ä¹Ÿå°±æ˜¯åœ¨è®­ç»ƒä¸­æ‰¾FixScale)
* [1802-Mixed Precision Training Of ConvNets Using Integer Operations-ICLR2018](https://arxiv.org/pdf/1802.00930.pdf)
  * 16bit Training
* [1805-Accurate & Efficient 2-bit QNN](https://www.semanticscholar.org/paper/ACCURATE-AND-EFFICIENT-2-BIT-QUANTIZED-NEURAL-Choi-Venkataramani/c3cb27f9ef7176658f37b607e75cc2c37f5e0ea8)
    * PACT + SAWB (Statistics-Aware Weight bining)
    * æ–‡ä¸­æœ‰Analyticåˆ†æPACTå’Œreluçš„è¡¨ç¤ºèƒ½åŠ›ä¸€è‡´
* [1808-Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss](https://arxiv.org/abs/1808.05779)
  * å­¦ä¹ é‡åŒ–çš„åŒºé—´å’Œéçº¿æ€§é‡åŒ–çš„é—´éš”
* [1905-hawq: hessian aware quantization of neuralnetworks with mixed-precision](https://arxiv.org/pdf/1905.03696.pdf)
    * [1911-V2](https://arxiv.org/pdf/1911.03852.pdf)

* Other Works
   * [1903-Training Quantized Network with Auxiliary Gradient Module](https://arxiv.org/abs/1903.11236)
    * é¢å¤–çš„fullPrecisionæ¢¯åº¦æ¨¡å—(è§£å†³residueçš„skip connectionä¸å¥½å®šçš„é—®é¢˜ï¼Œç›®çš„å‰å‘å®Œå…¨fix point)ï¼Œæœ‰å‡ åˆ†ç”¨ä¸€ä¸ªFPå»æ æ†èµ·ä½æ¯”ç‰¹ç½‘ç»œçš„æ„å‘³
  * [1901-Accumulation bit-width Scaling](https://arxiv.org/abs/1901.06588)
    * IBM ICLR 2019, æ‰¾Accumulatorå¯ä»¥å‹åˆ°å¤šå°‘ 

# C. (Full) Fixed-Point Training
> çº¯å®šç‚¹çš„è®­ç»ƒ,å¤§éƒ¨åˆ†éƒ½æ˜¯å¯¹ä¸€äº›ç»å…¸çš„éƒ¨åˆ†åšä¸€äº›ç®€åŒ–ã€‚ä»¥åŠå¯¹æ¢¯åº¦é‡åŒ–ä¹Ÿä¼šå½’å…¥æ­¤ç±»(ç›®çš„æ˜¯è®­ç»ƒçš„åŠ é€Ÿï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸ºäº†éƒ¨ç½²) 

* [1606-DoReFa](https://arxiv.org/abs/1606.06160)
  * ç®—æ˜¯æœ€å…ˆæå‡ºä½æ¯”ç‰¹è®­ç»ƒ
  * æ¯ä¸ªLayeræœ‰ä¸€ä¸ªScalingFactor
* [1802-WAGE - Training & Inference with Integers in DNN](https://arxiv.org/abs/1802.04680)
  * å…¨å®šç‚¹ï¼Œç®€åŒ–äº†å¾ˆå¤šéƒ¨åˆ†ï¼Œä½†æ˜¯ç²¾åº¦æŸå¤±æ¯”è¾ƒæ˜æ˜¾
* [1705-TernGrad](https://arxiv.org/abs/1705.07878)
  * å°†æ¢¯åº¦å®šç‚¹ä¸º3å€¼(ç”¨ç±»ä¼¼TWNçš„æ–¹å¼)ï¼Œç›®çš„æ˜¯å‡å°‘åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„é€šä¿¡ä¼ æ’­
* [1805-Scalable Methods for 8-bit Training of Neural Networks](https://arxiv.org/abs/1805.11046)
  * Intel(AIPG) NIPS2018 (WAG8)(RangeBN)
* [1812-Training Deep Neural Networks with 8-bit Floating Point Numbers](https://arxiv.org/abs/1812.08011)
  * FP8 - Training
* [1905-mixed precision training with 8-bit floating point](https://arxiv.org/abs/1905.12334)
  * wage all fp8
  * å¯¹æ¯”äº†rne(round2nearesteven)&sstochastic rounding 
* å…¨éƒ¨8bitè®­ç»ƒFlow - [Towards Unified INT8 Training for Convolutional Neural Network](https://arxiv.org/pdf/1912.12607.pdf)
	* SenseTime CVPR 2020


* Other Methods
  * [1812-Per-Tensor-Quantization of BackProp](https://arxiv.org/abs/1812.11732)
    * ICLR2019, Precision Assignment (å¥½å¤šæ•°å­¦å‡è®¾åˆ†æ),ç»™å‡ºäº†ä¸€ç§ç¡®å®šæ¯å±‚ä½å®½çš„æ–¹æ³•
  * [1812-Hybrid 8-bit Training](https://arxiv.org/abs/1812.08011)
    * FP8çš„åç»­ï¼Œå¯¹ä¸åŒçš„ç»„ä»¶æå‡ºä¸åŒçš„exponential bitä¸mantissa bitçš„åˆ’åˆ†æ–¹å¼

# Ideas

## Post-Training Quantization
* Important Weighing
  * é€šè¿‡è¯„ä¼°æ¯ä¸ªå‚æ•°å¯¹æœ€ç»ˆLossçš„é‡è¦ç¨‹åº¦ï¼Œä½œä¸ºå‰ªææˆ–æ˜¯é‡åŒ–çš„ä¾æ®ï¼Œæ¯”å¦‚[è¿™ç¯‡æ–‡ç« ](https://www.mdpi.com/1099-4300/18/12/449)  
* Knowledge Distillation
  * é€šè¿‡ä¸€ä¸ªé«˜ç²¾åº¦æ¨¡å‹ç»™ä½æ¯”ç‰¹æ¨¡å‹çš„Finetuneæä¾›SoftLabel
* Incremental & Iterative Quantization
  * GraduallyåšQuantizationï¼Œå‰è€…æ˜¯æ¯æ¬¡é€‰å–ä¸€éƒ¨åˆ†Weightï¼Œåè€…æ˜¯é€æ¸é™ä½æ¯”ç‰¹æ•°ç›®
* Analytical Correction
  * ä»è§£æçš„è§’åº¦å»åšä¸€äº›çŸ«æ­£æˆ–è€…æ˜¯é™åˆ¶,[ä»£è¡¨æ–‡ç« ](https://arxiv.org/abs/1810.05723)
* Data-Free Quantization
  * è¿‘å‡ å¹´å…´èµ·çš„ä¸€ä¸ªé¢†åŸŸ,å’ŒğŸ‘†çš„æ–¹æ³•æœ‰ä¸€äº›ç›¸å…³æ€§

## Quantize-Aware Training

> å®šç‚¹æ•°åŒ–ä¹‹åç›¸æ¯”äºæµ®ç‚¹æœ€å¤§çš„é—®é¢˜å°±æ˜¯**åŠ¨æ€èŒƒå›´**,ä¹Ÿæ˜¯ä¸»è¦çš„ä¼˜åŒ–ç›®çš„ã€€

* é™åˆ¶åŠ¨æ€èŒƒå›´
  * ä»è®­ç»ƒè§’åº¦å°±ä¸è¦è®©å‚æ•°åˆ†å¸ƒçš„å¤ªæ•£,å‡å°‘é€ æˆçš„å½±å“,æ¯”å¦‚[PACT](https://arxiv.org/abs/1805.06085)
  * è®­ç»ƒä¸­åŠ å¤§WeightDecay(L1 Norm)ä¹‹åè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å®šç‚¹æ•ˆæœä¼šæ›´å¥½
* ç»†ç²’åº¦çš„ç¡®å®šé‡åŒ–åŒºé—´
  * åŒä¸€åŒºé—´ä¸­ä¼šå–åŒä¸€ä¸ªRange,é€šè¿‡å‡å°‘è¢«åˆ’å…¥åŒä¸€åŠ¨æ€èŒƒå›´çš„å‚æ•°é‡æ¥å‡å°‘Clampçš„å‘ç”Ÿ
  * ä»Layer-Wiseåˆ°Channel-Wiseå†åˆ°Block-Wise
* ç»†ç²’åº¦çš„åˆ†é…bitæ•°
  * Mixed-Precisionçš„ä¸€ç³»åˆ—æ–¹æ³•
  * Hybrid FP8 - å¯¹A/Wé‡‡å–ä¸åŒçš„æŒ‡æ•°bitåˆ†é…
* æ‰©å¤§åŠ¨æ€èŒƒå›´
  * FP8 - 2æ¬¡å¹‚çš„æµ®ç‚¹æ•°
* å……åˆ†åˆ©ç”¨åŠ¨æ€èŒƒå›´
  * å¼•å…¥æµ®ç‚¹ScalingFactor,[è¿™ç¯‡](https://arxiv.org/abs/1712.05877)
  * éçº¿æ€§é‡åŒ–
* å­¦ä¹ é‡åŒ–ä¸­çš„å„ç§å€¼(ClippingValue/QuantizeInterval)
  * ç”¨äºæ›¿ä»£è§£ææ–¹æ³•æ±‚å‡ºæ¥çš„ä¸€äº›è§£æè§£(è§£ææ–¹æ³•çš„ä¸€äº›å‡è®¾ä¸æ˜¯å¾ˆç²¾ç¡®)
  * [PACT](https://arxiv.org/abs/1805.06085)å­¦ä¹ äº†Reluçš„ClippingValue
  * [KAISTçš„è¿™ç¯‡æ–‡ç« ](https://arxiv.org/abs/1808.05779)å­¦ä¹ äº†é‡åŒ–çš„
  * [TTQ](https://arxiv.org/pdf/1612.01064.pdf)å­¦ä¹ äº†ScalingFactor

### Fixed-Point Training

> ä¸€èˆ¬åªæœ‰æœ‰åŠ é€Ÿè®­ç»ƒçš„éœ€æ±‚çš„æ—¶å€™æ‰éœ€è¦é‡åŒ–æ¢¯åº¦,ä½†æ˜¯ç”±äºæ¢¯åº¦çš„ç‰¹æ®Šæ€§(åŠ¨æ€èŒƒå›´å¤§,ä¸”å¤§å€¼å°å€¼éƒ½å¾ˆé‡è¦)é‡‡ç”¨ä¸€èˆ¬çš„é‡åŒ–æ–¹å¼ä¸è¶³,ä¸€èˆ¬éœ€è¦é¢å¤–çš„è®¾è®¡;ä½†æ˜¯ğŸ‘†éƒ¨åˆ†çš„å¾ˆå¤šæ–¹æ³•çš„ç›®çš„æ˜¯ä¸ºäº†æ›´å¥½çš„é‡åŒ–,å°æ¬¡å¯¹è¯¥éƒ¨åˆ†ä¹Ÿæœ‰æŒ‡å¯¼ä»·å€¼

* é‡åŒ–æ¢¯åº¦
  * [TernGrad](https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning)ä¸»è¦æ˜¯åˆ©ç”¨äº†éšæœºRounding
  * FP8/Fp16 æ‰©å¤§åŠ¨æ€èŒƒå›´
  * æ›´ç»†ç²’åº¦çš„åˆ’åˆ†Range
* ç®€åŒ–è®­ç»ƒ
  * BN
    * RangeBN / L1BN
  * WAGEç®€åŒ–äº†å„ç§(çŒ®ç¥­äº†ç²¾åº¦)
* å…¨éƒ¨8bitè®­ç»ƒFlow - [Towards Unified INT8 Training for Convolutional Neural Network](https://arxiv.org/pdf/1912.12607.pdf)




## Others
* Fitting Weight or Activationï¼Ÿ
  * ä»äºŒå€¼åŒ–ç½‘ç»œå¼€å§‹ï¼Œå¾ˆå¤šanalyticalçš„æ±‚è§£æ–¹å¼å¾€å¾€æ˜¯åœ¨å‚æ•°åˆ†å¸ƒé«˜æ–¯çš„å‡è®¾å‰æä¸‹ï¼Œæœ€å°åŒ–é‡åŒ–åå‚æ•°ä¸é‡åŒ–å‰å‚æ•°çš„MSE
  * åç»­çš„å¦‚[è¿™ç¯‡](https://arxiv.org/abs/1907.05686)ï¼Œè®¤ä¸ºå‚æ•°å¯ä»¥å˜åŒ–ï¼Œéœ€è¦ä¿å­˜çš„æ˜¯æ¯ä¸€å±‚çš„è¾“å‡ºActivation
* æŠŠWAå‹ç¼©åˆ°èƒ½æ”¾åˆ°ç‰‡ä¸Šèƒ½å¤Ÿæ˜¾è‘—æå‡ç¡¬ä»¶è®¾è®¡æ€§èƒ½(æ˜¾è€Œæ˜“è§)
  * è¿™å¯èƒ½ä¹Ÿæ˜¯BNNç³»åˆ—çš„æ¯”è¾ƒç¹è£çš„åŸå› 
* Huge Batch Sizeå¯ä»¥å¸®åŠ©Binaryçš„è®­ç»ƒ(åŸç†ä¸Šå¯¹ä½æ¯”ç‰¹åŒç†?)
* Rounding Methods - Neareset/Stochastic/Biased Rounding
  * Stochastic Roundingå¾ˆå…³é”®ï¼Œå¯ä»¥ç”¨åˆ°æ›´ä½æ¯”ç‰¹çš„ä¿¡æ¯
  * ~~ä½†æ˜¯å¯¹ç¡¬ä»¶è®¾è®¡ä¸å¤ªå‹å¥½~~
  * ä¹Ÿæ˜¯[TernGrad](https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning)ä¹‹æ‰€ä»¥èƒ½workçš„åŸå› 
* ä»æ¨¡å‹å‹ç¼©è§’åº¦çœ‹ï¼Œç¡¬ä»¶éƒ¨ç½²è®­ç»ƒçš„å‡ å¤§éš¾ç‚¹(å’Œä¸å¤ªelegantçš„åœ°æ–¹)
  * ä¾èµ–å¤§batchå¯¹å­˜å‚¨è¦æ±‚é«˜
  * éšæœºroundingï¼Œç¡¬ä»¶éš¾å®ç°

### ç»å…¸çš„å»ºæ¨¡æ–¹å¼
* å¦‚[Post-Training 4bit](https://arxiv.org/abs/1810.05723)ä¸€æ–‡,å„ç§Analyticalæ±‚è§£çš„æ–¹æ³•éƒ½æ˜¯å°†é‡åŒ–è¿‡ç¨‹æŠ½è±¡ä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜
* (ä¸€èˆ¬éƒ½å‡è®¾è¾“å…¥çš„åˆ†å¸ƒä¸ºGauss),æœ€å°åŒ–é‡åŒ–å‰åå‚æ•°çš„MSE


### Stochastic Roudning Related

* Stochastic Roundingå¯¹ä¿è¯Convergenceé‡è¦
  * ä½†æ˜¯WRPNä½œè€…æ–‡ä¸­è¡¨ç¤ºè‡ªå·±åˆ©ç”¨äº†Full-Precision Gradæ‰€ä»¥ä¸å…³é”®
  * Scalable Methods for 8-bit Training of Neural Networksä¹Ÿæåˆ°äº† 
  * Mixed Precision Training With 8-bit Floating Pointæ–‡ä¸­å¯¹RNEå’ŒStochastic Roundingåšäº†ä¸€ä¸ªå¯¹æ¯”
    * æ¯”å¦‚imagenet Res18ï¼Œé—®é¢˜ä¸å¤§ï¼Œä½†æ˜¯Res50ä¼šæ˜æ˜¾Overfiting
    * ä½œè€…å½’å› ä¸ºNoisy Gradienté”™è¯¯æŒ‡å¼•äº†æ¨¡å‹çš„å‰è¿›æ–¹å‘ï¼Œweightè¢«é”™è¯¯æ›´æ–°äº†
    * åŒæ—¶å‘ç°L2èŒƒæ•°ä¼šæ¿€çƒˆå¢å¤§ï¼Œå¯¼è‡´äº†Gradientå˜å¾—æ›´åŠ Noisyï¼Œä»è€ŒVicious Circle
    * ç»“è®ºæ˜¯**RNE**çš„æ–¹æ³•å¯¹Gradientçš„Quantization Noiseä¸æ˜¯å¾ˆæœ‰æ•ˆ
    * è®¤ä¸ºstochastic roundingå‚è€ƒäº†è¢«ä¸¢æ‰bitçš„ä¿¡æ¯ï¼Œæ›´ç¨³å®š

### BN Related

* WAGEé¦–å…ˆæå‡ºBNæ˜¯è®­ç»ƒç“¶é¢ˆ
* [L1BN (Linear BN) ](https://arxiv.org/pdf/1802.09769.pdf)
    * å°†BNæ‰€æœ‰çš„æ“ä½œéƒ½è½¬åŒ–ä¸ºçº¿æ€§
* [RangeBN](https://arxiv.org/pdf/1802.09769.pdf)
    * å°†BNçš„Varè½¬åŒ–ä¸ºä¸€ä¸ª|Max-Min|*(1/sqrt(2ln(n)))
* å‡è®¾BNçš„running mean/varå·²ç»ç¨³å®š
    * ç„¶åæŠŠä»–å½“ä½œå¸¸æ•°æ¥è®¡ç®—


# Genre

> è¿™ä¸€éƒ¨åˆ†åˆ—ä¸¾äº†å¾ˆå¤šæ–‡ç« ï¼Œä½†æ˜¯å¾ˆå¤šå¹¶æ²¡æœ‰é˜…è¯»è¿‡

## BinaryåŠå…¶å»¶ç”³(æä½æ¯”ç‰¹)

> ä»ä¸€å¼€å§‹çš„BNNå»¶ç”³å¼€æ¥çš„ä¸€ç³»åˆ—workï¼ŒåŸºæœ¬éƒ½æ˜¯åˆ©ç”¨äº†äºŒå€¼ä¹‹åä¹˜æ³•å˜bitwiseï¼Œå¯¹ç¡¬ä»¶éƒ¨ç½²é‡‡ç”¨éä¼ ç»Ÿè¿ç®—å•å…ƒã€‚

* [BNN](https://arxiv.org/abs/1602.02830)
* [BinaryConnect](https://arxiv.org/abs/1511.00363)
* [TernaryNet(TWN)](https://arxiv.org/abs/1605.04711)
* [XNorNet](https://arxiv.org/pdf/1603.05279.pdf)
* [ABCNet](https://arxiv.org/abs/1711.11294)
* [WRPN-Intel-ICLR2018](https://openreview.net/pdf?id=B1ZvaaeAZ)
* [DoReFaNet](https://arxiv.org/pdf/1606.06160.pdf)
* [TTQ(Trained Ternary Quantization)](https://arxiv.org/pdf/1612.01064.pdf)
* [Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network using Truncated Gaussian Approximation](https://arxiv.org/abs/1810.01018)
* [Training Competitive Binary Neural Networks from Scratch](https://arxiv.org/abs/1812.01965)
* ~~æœ€åè¿™ç¯‡æ–‡ç« æ”¾åœ¨è¿™é‡Œåªæ˜¯ä¸ºäº†å‘Šè¯‰å¤§å®¶è¿™ä¸ªé¢†åŸŸåˆ°äº†2019å¹´è¿˜åœ¨è“¬å‹ƒå‘å±•~~

## é‡åŒ–æ–¹æ³•ï¼ˆä½æ¯”ç‰¹ï¼‰

> ä½¿ç”¨ç›¸å¯¹è¾ƒä¸ºä¼ ç»Ÿçš„æ¯”ç‰¹æ•°(4,6,8)ï¼Œåœ¨å…·ä½“é‡åŒ–æ–¹å¼ï¼Œä»¥åŠè®­ç»ƒæ–¹å¼å…¥æ‰‹

* [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877)
* [PACT](https://arxiv.org/abs/1805.06085)
* [Incremental Quantization](https://ieeexplore.ieee.org/document/4476718/)
* [Model compression via distillation and quantization](https://arxiv.org/abs/1802.05668)
* [Training Deep Neural Networks with 8-bit Floating Point Numbers](https://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers.pdf)

## ç†è®ºåˆ†æ
* [Training Quantized Nets: A Deeper Understanding](https://arxiv.org/abs/1706.02379)
* [Towards The Limits Of Network Quantization](https://openreview.net/pdf?id=rJ8uNptgl)
* [Accumulation bit-width Scaling](https://arxiv.org/abs/1901.06588)
* [Per-Tensor-Quantization of BackProp](https://arxiv.org/abs/1812.11732)
* [Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets](https://arxiv.org/abs/1903.05662)
* [An Empirical study of Binary Neural Networks' Optimisation](https://openreview.net/pdf?id=rJfUCoR5KX)
* [Scalable Methods for 8-bit Training of Neural Networks | Part 5](https://arxiv.org/abs/1805.11046)


## ~~å¥‡æŠ€æ·«å·§~~

* [ernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning](https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning)
* [Deep Learning with Low Precision by Half-wave Gaussian Quantization](https://arxiv.org/abs/1702.00953)
* [Learning low-precision neural networks without Straight-Through Estimator (STE)](https://arxiv.org/abs/1903.01061)
* [SWALP: Stochastic Weight Averaging in Low-Precision Training](https://arxiv.org/abs/1904.11943)
* [Analysis Of Quantized MOdels-ICLR2019](https://openreview.net/forum?id=ryM_IoAqYX)
* [Training Quantized Network with Auxiliary Gradient Module](https://arxiv.org/abs/1903.11236)
* [Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss](https://arxiv.org/abs/1808.05779)
* [ReLeQ: An Automatic Reinforcement Learning Approach  for Deep Quantization of Neural Networks](http://www-users.cselabs.umn.edu/classes/Spring-2019/csci8980/papers/releq.pdf)
* [And the Bit Goes Down: Revisiting the Quantization of Neural Networks](https://arxiv.org/abs/1907.05686)
* [Ternary MobileNets via Per-Layer Hybrid Filter Banks](https://arxiv.org/abs/1911.01028)
* [Effective Training of Convolutional Neural Networks with Low-bitwidth Weights and Activations](https://arxiv.org/abs/1908.04680)
* [MoBiNet: A Mobile Binary Network for Image Classification](https://arxiv.org/abs/1907.12629)

# Docs

> çœ‹ä¸€ä¸‹å¤§å…¬å¸ä¸»æµçš„å‹ç¼©å·¥å…·éƒ½æä¾›äº†ä»€ä¹ˆåŠŸèƒ½

## Tensorflow Lite

> tf.contrib.quantize & [Tensorflow Lite](https://www.tensorflow.org/lite/guide)

* æä¾›äº†ä¸€ä¸ª[Post-Training-Quantizeçš„å·¥å…·](https://www.tensorflow.org/lite/performance/post_training_quantization)
  * çœ‹ä¸Šå»æ˜¯å¾ˆç›´æ¥çš„Quantizeæ²¡æœ‰ç”¨åˆ°ä»€ä¹ˆæŠ€å·§ï¼Œæ ‡å‡†æ•°æ®æ ¼å¼ä¹‹é—´çš„è½¬å˜(float16/int8)  **æ²¡æœ‰è‡ªå®šä¹‰çš„æ•°æ®æ ¼å¼**
  * æ–‡æ¡£ä¸­ç›´æ¥å†™åˆ° ```If you want Higher Performance, Use Quantize-aware Training```
  * æ²¡æœ‰Finetuneçš„è¿‡ç¨‹ï¼Ÿ
* [Quantize-Aware Training](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/contrib/quantize)
  * åªæ”¯æŒéƒ¨åˆ†ç½‘ç»œç»“æ„(åˆç†)ä»¥å·ç§¯ä¸ºä¸»ï¼Œè¿˜æœ‰ä¸€äº›RNN
  * è¿™é‡Œé»˜è®¤Foldäº†Convå’ŒBN(æˆ‘ä»¬æ‰€è¯´çš„MergeConvBNs)
  * æœ‰ä¸€ä¸ªTOCO(Tf Lite Optimizing Converter)å·¥å…·å¯ä»¥ç›´æ¥å°†è®­ç»ƒå¥½çš„FrozenGraphè½¬åŒ–ä¸ºçœŸæ­£çš„å®šç‚¹æ¨¡å‹

## PyTorch

> [Quantization Tool](https://pytorch.org/docs/stable/quantization.html?highlight=quantize)
* **QNNPack**
* æ”¯æŒPerTensorå’ŒPerChannelçš„é‡åŒ–ï¼Œé‡‡ç”¨å¸¦zeropointçš„rounding
* Quantize Aware Training at ```torch.nn.qat torch.nn.intrinsic.qat```
* æä¾›äº†å¾ˆå¤šObserver
* æ”¯æŒSymmetricalå’ŒAsymmetricalçš„é‡åŒ–





---

# Groups (Low-Bit Training)

* Intel (AIPG)
* KAIST (Korea)
* IBM
* Kaust (è¿ªæ‹œçš„ä¸€ä¸ªå­¦æ ¡...)

# TODO

* å®ç°å¯¹BNçš„é­”æ”¹
  * é¦–å…ˆç ”ç©¶å®ƒçš„å®šç‚¹Beahaviour
    * ~~è¿˜éœ€è¦å†è¿‡ä¸€è¿‡æ–‡çŒ®ä¸­ï¼Œç»Ÿè®¡ä»¥ä¸‹ç›®å‰å¯¹BNçš„è®¤è¯†(é™¤äº†ä¸èƒ½å®šç‚¹ä¹‹å¤–ï¼Œå¤§å®¶éƒ½æ˜¯æ€ä¹ˆå¤„ç†çš„)~~
* å®ç°Stochastic rounding
* å®ç°PACT
* å¯¹ç›®å‰çš„å‡ ç§Rangeæ–¹æ³•å®ç°åˆ†ç»„
* å¯¹WAåšClampå®ç°ï¼Œä½œä¸ºè¶…å‚æ•°åŠ å…¥
* (?)  èƒ½å¦ä»TernaryNetè¿™ä¸€ç³»åˆ—ä¸­æå–å‡ºä¸€äº›å¯ä»¥å‚è€ƒçš„ä¼˜åŒ–ç‚¹ï¼ˆæ¯”å¦‚è®­ç»ƒå‡ºä¸€ä¸ªRangeï¼‰

# References

* [Awesome-Model-Compression](https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression/blob/master/Paper/PaperByConference.md)
* [Blog N0.0](https://blueardour.github.io/2019/04/29/model-compression-summary.html)
* [TensorBoard Lite Doc](https://www.tensorflow.org/lite/performance/post_training_quantization)
* [Distiller Doc](https://nervanasystems.github.io/distiller/quantization.html)

